{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Update Pipeline\n",
    "\n",
    "This checks the RCSB PDB for entries since the last query.  New entries are downloaded, parsed, and added to the existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today is: 2022-06-21\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "#\n",
    "# Run the functions in the final cell to update the PDB dataframe\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Last Update Date\n",
    "\n",
    "def find_last_update():  \n",
    "    \n",
    "    # mm/dd/y\n",
    "    global today_YYYYMMDD\n",
    "    today_YYYYMMDD = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    list_of_previous_updates = [file for file in sorted(glob.glob('PDB_list_CSVs/*-additions.txt'))]  #for full set, look in subdirs \n",
    "\n",
    "    global last_update\n",
    "    last_update = list_of_previous_updates[-1][14:24] #\"2022-04-11\"\n",
    "\n",
    "    print(\" - last PDB database update was:\", last_update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The json query string is the actual query that works, { -> %3A, : -> %7B etc. I couldn't get url encoding to match.\n",
    "\n",
    "#last_update is date pulled from a file name; inserted into query\n",
    "#eg. last_update = \"2022-04-11\"\n",
    "\n",
    "\n",
    "def pdb_request_list():\n",
    "    my_json_query_str = f'%7B\"query\"%3A%7B\"type\"%3A\"group\"%2C\"logical_operator\"%3A\"and\"%2C\"nodes\"%3A%5B%7B\"type\"%3A\"terminal\"%2C\"service\"%3A\"text\"%2C\"parameters\"%3A%7B\"attribute\"%3A\"exptl.method\"%2C\"operator\"%3A\"exact_match\"%2C\"negation\"%3Afalse%2C\"value\"%3A\"X-RAY%20DIFFRACTION\"%7D%7D%2C%7B\"type\"%3A\"terminal\"%2C\"service\"%3A\"text\"%2C\"parameters\"%3A%7B\"attribute\"%3A\"rcsb_accession_info.initial_release_date\"%2C\"operator\"%3A\"greater\"%2C\"negation\"%3Afalse%2C\"value\"%3A\"{last_update}\"%7D%7D%5D%2C\"label\"%3A\"text\"%7D%2C\"return_type\"%3A\"entry\"%2C\"request_options\"%3A%7B\"paginate\"%3A%7B\"start\"%3A0%2C\"rows\"%3A100000%7D%2C\"scoring_strategy\"%3A\"combined\"%2C\"sort\"%3A%5B%7B\"sort_by\"%3A\"score\"%2C\"direction\"%3A\"desc\"%7D%5D%7D%7D'\n",
    "\n",
    "    #Query PDB using REST API\n",
    "    #Search for X-Ray structures deposited since last date\n",
    "\n",
    "    global page\n",
    "    page = requests.get(f\"https://search.rcsb.org/rcsbsearch/v2/query?json={my_json_query_str}\")\n",
    "    #print(f\"https://search.rcsb.org/rcsbsearch/v2/query?json={my_json_query_str}\")\n",
    "    print(' - PDB qeury for database update is now complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last PDB database update was: 2022-06-03\n",
      "PDB qeury complete\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pdbs_to_csv():\n",
    "    #Parse response from PDB\n",
    "    soup = BeautifulSoup(page.text, \"html\")\n",
    "    jason = json.loads(soup.body.get_text())\n",
    "\n",
    "    #json.dumps(jason)\n",
    "    \n",
    "    #Generate list of PDB names to populate CSV file\n",
    "\n",
    "    csv_list = [] \n",
    "    for mini_dic in jason['result_set']:\n",
    "        csv_list.append(mini_dic[\"identifier\"])\n",
    "        \n",
    "    #Write CSV File containing IDs of new PBB additions\n",
    "    with open(f'./PDB_list_CSVs/{today_YYYYMMDD}-new-PDB-additions.txt', 'w') as f:\n",
    "        write = csv.writer(f)  \n",
    "        write.writerow(csv_list)\n",
    "\n",
    "    #list of new PDBs \n",
    "    #os.environ['PDB_LIST'] = f'./PDB_list_CSVs/{today_YYYYMMDD}-new-PDB-additions.txt'\n",
    "    global pdb_csv_list\n",
    "    pdb_csv_list = f'./PDB_list_CSVs/{today_YYYYMMDD}-new-PDB-additions.txt'\n",
    "    \n",
    "    print(f\" - there are {len(csv_list)} new PDBs since the last update\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - last PDB database update was: 2022-06-03\n",
      " - PDB qeury for database update is now complete\n",
      " - there are 286 new PDBs since the last update\n",
      " - the new PDB entries have been downloaded and unzippped\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdb_getter():\n",
    "    \n",
    "    #Fetch the PDBs using the shell script provided by the RCSB PBD\n",
    "    #https://www.rcsb.org/docs/programmatic-access/batch-downloads-with-shell-script\n",
    "    subprocess.run(['./PDB_batch_download.sh', '-f', f'{pdb_csv_list}', '-o' './PDB_depot', '-p'], stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    #Unzip them\n",
    "    #Run in shell via Popen to expand wild card\n",
    "    subprocess.Popen('gunzip -d ./PDB_depot/*pdb.gz', shell = True,\n",
    "                     stdout=subprocess.PIPE,\n",
    "                     stderr=subprocess.PIPE,\n",
    "                     text=True)\n",
    "    \n",
    "    \n",
    "    print(\" - the new PDB entries have been downloaded and unzippped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pdb_dataframer():\n",
    "    #Populate dataframe of new PDB info\n",
    "    global df_new\n",
    "    df_new = pd.DataFrame(columns=[\"Resolution\", \"Completeness\", \"I_sigma\", \"R_value\", \"R_free\", \"Detector\", \"Det_type\", \"Optics\"])\n",
    "\n",
    "\n",
    "    #Add date - I can drop anything before... 2002? 20 years is a lot and the early entries are garbage/useless\n",
    "\n",
    "                                        #only the new PDBs (not yet moved to subdirs)\n",
    "    for filename in glob.glob('PDB_depot/*.pdb'):  #for full set, look in subdirs \n",
    "        with open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "            resolution = completeness = i_sigma = r_value = r_free = detector = det_type = optics = \"NULL\"  #set as null in case line not present\n",
    "\n",
    "            pdb_id = filename[-8:-4]\n",
    "            for line in f:\n",
    "\n",
    "                #Exit condition, 50% drop in time\n",
    "                #works without strip? remove other line strips for speed 30% drop in time\n",
    "                if line.startswith(\"ATOM\"):\n",
    "                    break\n",
    "\n",
    "                #Resolution\n",
    "                if line.startswith(\"REMARK   3   RESOLUTION RANGE HIGH (ANGSTROMS)\"):\n",
    "                    resolution = line.split(\":\")[1].strip()\n",
    "\n",
    "                #Completeness\n",
    "                if line.startswith(\"REMARK   3   COMPLETENESS FOR RANGE        (%)\"):\n",
    "                    completeness = line.split(\":\")[1].strip()         \n",
    "\n",
    "                #I/sigma\n",
    "                if line.startswith(\"REMARK 200  <I/SIGMA(I)> FOR THE DATA SET\"):\n",
    "                    i_sigma = line.split(\":\")[1].strip()      \n",
    "\n",
    "                #R-Value\n",
    "                if line.startswith(\"REMARK   3   R VALUE     (WORKING + TEST SET)\"):\n",
    "                    r_value = line.split(\":\")[1].strip()\n",
    "\n",
    "\n",
    "                #R-Free\n",
    "                if line.startswith(\"REMARK   3   FREE R VALUE\"):\n",
    "                    if line.split(\":\")[0].strip().endswith(\"VALUE\"):  #need to match end here\n",
    "                        r_free = line.split(\":\")[1].strip()\n",
    "\n",
    "\n",
    "                #Detector Model (Pilatus 6M, Eiger 16M etc)\n",
    "                if line.startswith(\"REMARK 200  DETECTOR MANUFACTURER\"):\n",
    "                    detector = line.split(\":\")[1].strip()\n",
    "                    if \";\" in detector:\n",
    "                        detector = line.split(';')[0].strip()  #some lines contain 2 fields, 2nd redundant\n",
    "\n",
    "                #Detector Type (Pixel, CCD, etc)\n",
    "                if line.startswith(\"REMARK 200  DETECTOR TYPE\"):\n",
    "                    det_type = line.split(\":\")[1].strip()\n",
    "\n",
    "\n",
    "                #Optics (Mirrors)\n",
    "                if line.startswith(\"REMARK 200  OPTICS\"):\n",
    "                    optics = line.split(\":\")[1].strip()\n",
    "\n",
    "\n",
    "\n",
    "            #print(pdb_id, resolution, completeness, i_sigma, r_value, r_free, detector, det_type, optics)\n",
    "            df_new.loc[pdb_id] = [resolution, completeness, i_sigma, r_value, r_free, detector, det_type, optics]\n",
    "            \n",
    "    print(\" - new dataframe for concatenation is constructed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_dataframe_updater():\n",
    "\n",
    "    #Load the existing (full) dataframe of PDB info\n",
    "    df_full = pd.read_feather('/Users/jesse/TDI_bootcamp/capstone_PDB/dataframe_feather/1_initial_dataframe_163K_full.feather')\n",
    "    \n",
    "    #Concat old and new dataframe\n",
    "    df_catted = pd.concat([df_new.reset_index(), df_full]) \n",
    "\n",
    "    #remove old indices column\n",
    "    df_catted = df_catted.reset_index().drop(['level_0'], axis=1) \n",
    "    \n",
    "    #Write the new, updated PDB DF to feather\n",
    "    df_catted.to_feather('./dataframe_feather/1_updated_dataframe.feather')\n",
    "    \n",
    "    print(\"\"\" - the prexisting datframe has been updated and written to this location:\\\n",
    "             \\n \\t ./dataframe_feather/1_updated_dataframe.feather\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_To-Do.ipynb                      3_Data_Regression.ipynb\r\n",
      "0_DataFrame_Initial_Assembly.ipynb  Capstone_Flowchart.pdf\r\n",
      "1_DataFrame_Update_Pipeline.ipynb   \u001b[31mPDB_batch_download.sh\u001b[m\u001b[m*\r\n",
      "2_DataFrame_Cleaning.ipynb          README.md\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pdb_file_cataloger():\n",
    "    #Move the new PDB files into the 1xxx/ etc archives\n",
    "    \n",
    "    #backticks don't actually need curly braces for $VAR expansion\n",
    "    #Double quotes do\n",
    "    \n",
    "    #command = \"\"\"for n in $(seq 1 9); do for i in `ls PDB_depot/${n}*.pdb`; do mv $i  \"PDB_depot/${n}xxx_pdbs\"; done; done\"\"\"\n",
    "    command = \"\"\"for n in $(seq 1 9); do\n",
    "    for i in `ls PDB_depot/${n}*.pdb`; do\n",
    "        mv $i  \"PDB_depot/${n}xxx_pdbs\";\n",
    "    done\n",
    "    done\"\"\"\n",
    "    \n",
    "    subprocess.Popen(command, shell = True,\n",
    "                     stdout=subprocess.PIPE,\n",
    "                     stderr=subprocess.PIPE,\n",
    "                     text=True)\n",
    "    \n",
    "    print(\" - the new pdbs have been cataloged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run these functions to update the dataframe\n",
    "\n",
    "print(\"today is:\", today_YYYYMMDD)\n",
    "\n",
    "find_last_update()\n",
    "pdb_request_list()\n",
    "new_pdbs_to_csv()\n",
    "pdb_getter()\n",
    "new_pdb_dataframer()\n",
    "old_dataframe_updater()\n",
    "new_pdb_file_cataloger()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
